{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* [Outline](../0_Introduction/0_introduction.ipynb)\n",
    "* [Glossary](../0_Introduction/1_glossary.ipynb)\n",
    "* 2. [Mathematical Background](2_0_introduction.ipynb)\n",
    "    * Previous: [2.13 Spherical Trigonometry](2_13_sampling_theory.ipynb)\n",
    "    * Next: [2.x Further Reading and References](2_x_further_reading_and_references.ipynb)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize as opt\n",
    "import scipy.signal as scg\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Mathematics of the CLEAN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give the mathematical details behind the implementation of the CLEAN algorithm. In particular we present deconvolution as an iterative $\\chi^2$ minimisation of a linear system of equations. You should review the notation for linear systems given in [$\\S$ 2.10.3 &#10142;](2_10_linear_algebra.ipynb). Although, by this stage, you should have the necessary mathematics to understand this section, it is meant to supplement the material given in [$\\S$ 6 &#10142;](../6_Deconvolution) and is best read after you have become familiar with the basic concepts behind imaging and deconvolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14.1 Imaging as an iterative $\\chi^2$ minimisation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imaging problem can be formulated as a linear system of the form\n",
    "$$ S F I = V_{obs} + \\epsilon, \\quad \\mbox{where} \\quad \\epsilon \\sim \\mathcal{N}(0,\\Sigma).$$\n",
    "Here $S$ is the sampling operator, $F$ the Direct Fourier transform and $I$ is the image we are after. The assumption that $\\epsilon$ is Gaussian noise allows us to form the $\\chi^2$ distribution as\n",
    "\n",
    "$$ \\chi^2 = (S F I - V_{obs})^H W (S F I - V_{obs}), $$\n",
    "\n",
    "where $W$ are the imaging weights. As already mentioned in $\\S$ 2.10.3 (and elsewhere I am sure???), in interferometry, $W$ is not necessarily just the inverse of the covariance matrix $\\Sigma^{-1}$. As will be demonstrated below the weights can actually be chosen to enhance certain desirable features in the image. For example, we could choose to implement uniform weighting which would down weight short baselines resulting in a sharper point spread function (denoted $I^{PSF}$) with smaller sidelobes. The downside of such a weighting scheme is that we lose signal to noise in the reconstructed dirty image (denoted $I^{D}$). We will give some examples below. For now our goal is to figure out how to find the image $I$ which minimises the $\\chi^2$. Recall that this can be achieved by solving the normal equations. With the model as given above the gradient (also the Jacobian) is given by\n",
    "\n",
    "$$ \\partial_x \\chi^2 = \\mathbb{J}(I) = F^H S^H W (S F I - V_{obs}). $$\n",
    "\n",
    "Similarly the Hessian follows on taking the second derivative\n",
    "\n",
    "$$ \\partial^2_x \\chi^2 = \\mathbb{H} = F^H S^H W S F. $$\n",
    "\n",
    "The normal equations result from setting the gradient of the $\\chi^2$ to zero i.e.\n",
    "\n",
    "$$ F^H S^H W S F I = F^H S^H W V_{obs}. $$\n",
    "\n",
    "Notice how the LHS is just the Hessian operating on $I$. The RHS, on the other hand, is just $I^{D}$ so that we can write the equation more succintly as\n",
    "\n",
    "$$ \\mathbb{H} I = I^{D}.  $$\n",
    "\n",
    "Here $\\mathbb{H}$ is a circulant matrix with each successive row being a right shifted version of $I^{PSF}$. Recalling (from $\\S$ 2.10.4) that multiplication with a circulant matrix actually performs a discrete convolution, we clearly see that $I^D$ is the true sky convolved with the PSF of the instrument i.e. $I^{D} = I^{PSF} * I$. In principle the solution to the normal equations is given by\n",
    "\n",
    "$$ I = \\mathbb{H}^{-1} I^D, $$\n",
    "\n",
    "showing why we can think of the solution to the normal equations as a deconvolution. Unfortunately, for a sparsely sampled array, the sampling matrix $S$ will not have full column rank and therefore can't be inverted. This forces the use of approximate methods to solve the normal equations. The CLEAN algorithm does this by using an iterative procedure similar to Newton's method i.e. with an update rule of the form\n",
    "\n",
    "$$ I_{i+1} = I_i + g \\tilde{H}^{-1} J(I_i), $$\n",
    "\n",
    "where $\\tilde{H}$ is an invertible approximation to the Hessian matrix and the loop gain $g$ controls the step size. \n",
    "\n",
    "In imaging the iterative solution is usually implemented using major and minor cycles. To see how this works first note that, for a given model image $I^{M}$, the Jacobian is just the residual image i.e. \n",
    "\n",
    "$$ J(I^{M}) = F^H S^H W (S F I^M - V_{obs}) = I^{R}. $$\n",
    "\n",
    "Thus one iteration of the optimisation method would be given by \n",
    "\n",
    "$$ I^M_{i+1} = I^M_{i} + g \\tilde{H}^{-1} I^R_{i}, $$\n",
    "\n",
    "where $\\tilde{H}$ is constructed by invoking a number of a priori assumptions. Recall that an ideal interferometer (i.e. one with complete $uv$-coverage) will have a delta function as its PSF. This suggests that we can, in principle, eliminate the corrupting effects of the instrument by assuming that the PSF is a delta function. This is equivalent to a diagonal approximation of the Hessian matrix. If we also assume that the PSF is spatially invariant all the elements on the diagonal will be the same and the Hessian reduces to a single number viz. the value of $I^{PSF}$ at the center. This value is also known as the sum of weights $w_{sum}$ and we will see where this name comes from below. With these two assumptions we can construct \n",
    "\n",
    "$$ \\tilde{H} = w_{sum} = \\mbox{peak}(I^{PSF}). $$\n",
    "\n",
    "This allows us to construct what is known as the principal solution $\\hat{I}^D$ as\n",
    "\n",
    "$$ \\hat{I}^D = \\tilde{H}^{-1} I^D, \\qquad \\hat{I}^{PSF} = \\tilde{H}^{-1} I^{PSF}. $$\n",
    "\n",
    "Note that we are using a hat to denote normalisation by the peak of the PSF. The principal solution is basically just the dirty image normalised to have units which are comparable to the model image. To apply the update step we start by searching for the peak in $I^D$. This peak will contain flux from the pixel in the model image centered at the same location, as well as those contributed by the the sidelobes of sources at different locations. This is the reason why we have to use a loop gain of $g < 1$, otherwise we will be adding sidelobes of the other sources to our model image. Once we have performed the update step we can compute the new residual image (i.e. evaluate the Jacobian) and repeat until some prespecified convergence criterion has been reached. This is known as one major cycle during CLEAN deconvolutions. Since it involves switching from the image domain to visibilities and back again, it is quite expensive operation. As a result the algorithm is usually implemented differently and this is where the minor cycle comes in. Major cycles evaluate the Jacobian while minor cycles implement more efficient (but less accurate) updates to the model image. Basically the minor cycle does multiple updates of the model image to avoid switching between domains so frequently. This requires performing the subtraction (equivalently computing the Jacobian or updating the residual image $I^R$) in the image domain and can be implemented approximately by subtracting out the PSF centered at the location of the current brightest pixel from the entire image. Keep in mind that, since we are evaluating the PSF on a finite grid, this inevitably introduces discretisation errors which must be corrected for in the major cycle. Actually all of this is best illustrated with a simple example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14.2 CLEAN illustration in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim is to develop the CLEAN algorithm in one dimension. We start by implementing the basic Hogbom CLEAN algorithm (which implements the minor cycle) and then extend it to a full Cotton-Schwabb CLEAN (which evaluates the Jacobian in visibility space). \n",
    "\n",
    "Recall that an interferometer measures the Fourier transform of the sky brightness distribution. Assuming an unpolarised sky and an ideal interferometer this is captured by the Van Cittert-Zernike theorem i.e.\n",
    "\n",
    "$$ V(u,v) = \\int A(l,m) I(l,m)e^{-2 \\pi i (ul + vm)}dldm, $$\n",
    "\n",
    "where $A(l,m)$ is the primary beam pattern, $I(l,m)$ is the sky brightness ditribution and $V(u,v)$ the visibilities measured by the interferometer. To illustrate the main concepts behind deconvolution we will study the following simplified 1D model\n",
    "\n",
    "$$ V(u) = \\int I(l)e^{-2 \\pi i ul} dl = \\mathcal{F}(I), $$\n",
    "\n",
    "where the $\\mathcal{F}$ operator represents the Fourier transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by constructing a sky model containing only delta functions (this would correspond to unresolved point sources). Suppose our interferometer has a maximum field of view of $l \\in [-1,1]$ in which there are only two point sources located at $l_1 = -0.15$ and $l_2 = 0.25$ with a total intensity of $I_{s_1} = 1$Jy and $I_{s_2} = 2.5$Jy respectively. Such a sky model can be parametrised as\n",
    "\n",
    "$$ I(l) = I_{s_1} \\delta(l - l_1) + I_{s_2} \\delta(l - l_2).  $$\n",
    "\n",
    "The visibilities corresponding to this sky model are easy to compute analytically. They are\n",
    "\n",
    "$$ V(u) = I_{s_1} e^{-2 \\pi i u l_1} + I_{s_2} e^{-2 \\pi i u l_2}.  $$\n",
    "\n",
    "This can be visualised as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 300 # the maximum number of \"baselines\" that can be sampled\n",
    "u = np.linspace(-20,20,N) # a grid of \"baselines\"\n",
    "\n",
    "Npix = 300 # the number of \"pixels\" in the image\n",
    "lmin = -0.5 \n",
    "lmax = 0.5\n",
    "l = np.linspace(lmin,lmax,Npix) # The region where we want to reconstruct the image\n",
    "\n",
    "#deltal = l[1]-l[0]\n",
    "\n",
    "Is1 = 1 # true flux of source one\n",
    "Is2 = 2.5 # true flux of second source\n",
    "\n",
    "# Set locations of sources (also ensuring they are at pixel centres)\n",
    "l1 = l[np.argwhere(l < -0.15)[-1]]\n",
    "l2 = l[np.argwhere(l < 0.25)[-1]]\n",
    "\n",
    "# Compute the anaytic form of the visibilities\n",
    "V = Is1*np.exp(-2*np.pi*1.0j*u*l1) + Is2*np.exp(-2*np.pi*1.0j*u*l2)\n",
    "\n",
    "# Plot the visibilities\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].plot(u,np.real(V))\n",
    "ax[0].set_xlabel('$u$',fontsize=18)\n",
    "ax[0].set_ylabel('$real(V(u))$',fontsize=18)\n",
    "ax[0].set_xlim(-20.,20.)\n",
    "ax[0].set_ylim(-5,5)\n",
    "\n",
    "ax[1].plot(u,np.imag(V))\n",
    "ax[1].set_xlabel('$u$',fontsize=18)\n",
    "ax[1].set_ylabel('$imag(V(u))$',fontsize=18)\n",
    "ax[1].set_xlim(-20.,20.)\n",
    "ax[1].set_ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the theoretical signal corresponding to our sky model (over the finite domain over which we can make measurements). Unfortunately an interferometer only measures this signal at a finite number of locations in the $uv$-plane. This can be expressed mathematically by including a sampling function $S(u)$ in our measurement equation as follows\n",
    "\n",
    "$$ V^{obs}(u) = S(u) \\int I(l)e^{-2 \\pi i ul} dl = S \\mathcal{F} I, $$\n",
    "\n",
    "where $S(u) = 1$ at locations where we have measurements and $S(u) = 0$ otherwise. This results in a discretised linear model in the same form as we have above. We are going to start by considering the noiuse free case in which all observations are equally weighted (i.e. so called naturally weighted) and give a demonstration of the effect that the weighting function has right at the end. Inverting this equation gives what is known as the dirty image $I^D$\n",
    "\n",
    "$$I^D = \\mathcal{F}^{-1} S \\mathcal{F} I = I^{PSF} * I, \\quad \\mbox{where} \\quad I^{PSF} = \\mathcal{F}^{-1} S, $$\n",
    "\n",
    "and we have employed the convolution theorem in the last step. From this it should be clear that the image $I^D$ reconstructed by naively applying the inverse Fourier transform to the observed visibilities $V^{obs}$ is the convolution of the true image with the point spread function (PSF) of the instrument viz. $I^{PSF}$. For this reason $I^D$ is referred to as the dirty image. Deconvolution is the process by which we attempt to remove the corrupting affects of the instrument in order to reconstruct the true image of the sky. Before we can illustrate the deconvolution process we will need to simulate this affect on our model. The code snippet below does exactly that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices at which we measure V\n",
    "Nsample = 201\n",
    "Sindices = np.sort(np.unique(np.random.randint(0, N, Nsample))) # unique used to ensure we dont sample any given point more than once \n",
    "Nsample = Sindices.size # true number of sampels\n",
    "\n",
    "# Create sampling function\n",
    "S = np.zeros(N)\n",
    "S[Sindices] = 1.0\n",
    "\n",
    "# This is to compare DFT with FFT\n",
    "S2 = np.ones(Nsample)\n",
    "Vobs2 = V[Sindices]\n",
    "\n",
    "\n",
    "# Get the observed visibilities\n",
    "Vobs = np.zeros(N)\n",
    "Vobs[Sindices] = V[Sindices]\n",
    "\n",
    "# Construct FT kernel\n",
    "F = np.exp(-2.0*np.pi*1.0j*np.outer(u,l))\n",
    "\n",
    "F2 = np.exp(-2.0*np.pi*1.0j*np.outer(u[Sindices],l)) # for comparison with FFT\n",
    "lpsf = np.linspace(2*lmin,2*lmax,2*N+1)\n",
    "F2psf = np.exp(-2.0*np.pi*1.0j*np.outer(u[Sindices],lpsf))\n",
    "\n",
    "# Do the iFT on S (recal FT is Hermitian FF^H = I => F^H = F^{-1})\n",
    "Ipsf = np.real(np.dot(F.conj().T,S))\n",
    "\n",
    "Ipsf2 = np.real(np.dot(F2psf.conj().T,S2)) # for comparison with FFT\n",
    "\n",
    "# Get the dirty image\n",
    "ID = np.real(np.dot(F.conj().T,Vobs))\n",
    "ID2 = np.real(np.dot(F2.conj().T,Vobs2)) # for comparison with FFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have computed two versions of both $I^D$ and $I^{PSF}$ in the above code snippet. This is simply to illustrate the effects of aliasing when using a regular grid (as necessitated by use of the FFT algorithm). Lets first have a look at the what we would get if we sampled the visibilities on a regular grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].plot(l,Ipsf)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$I^{PSF}$',fontsize=18)\n",
    "ax[0].set_xlim(lmin,lmax)\n",
    "#ax[0].set_ylim(-5,5)\n",
    "\n",
    "ax[1].plot(l,ID)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^D$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)\n",
    "#ax[1].set_ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure on the left shows the PSF that results from imperfect sampling in the \"$uv$-plane\". We should expect to see something that resembles a sinc function (why?). On the right is the dirty \"image\" we would see if we did not account for aliasing. Use of the FFT inevitably requires the use of an anti-aliasing filter too. In this tut we get around that by using the irregularly sampled version of $V^{obs}$ (similar to using the dirct Fourier transform in imaging). The figures below show the anti-aliased version of $I^D$. Note that we have also computed $I^{PSF}$ out to double the distance as compared to $I^D$. This will be reuired for the deconvolution.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].plot(lpsf,Ipsf2)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$I^{PSF}$',fontsize=18)\n",
    "ax[0].set_xlim(2*lmin,2*lmax)\n",
    "#ax[0].set_ylim(-5,5)\n",
    "\n",
    "ax[1].plot(l,ID2)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^D$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)\n",
    "#ax[1].set_ylim(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are clearly two main peaks visible in $I^{D}$ but the units seem all wrong. Normalising both by the max of the PSF we see something that is closer to the original input model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPSF = Ipsf2.max()\n",
    "\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].plot(lpsf,Ipsf2/maxPSF)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$I^{PSF}$',fontsize=18)\n",
    "ax[0].set_xlim(2*lmin,2*lmax)\n",
    "\n",
    "\n",
    "ax[1].plot(l,ID2/maxPSF)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^D$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We are now ready to implement the Hogbom CLEAN. The function below implements the Hogbom CLEAN algorithm in 1D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hogbom_CLEAN(IR,Ipsf,gamma=0.1,tol=0.1,maxiter=200,restore=False):\n",
    "    # Initialise\n",
    "    i = 1\n",
    "    Npix = IR.size\n",
    "    Npsfmax = np.argwhere(Ipsf==1.0)\n",
    "    IM = np.zeros([Npix])\n",
    "    # Find the first peak\n",
    "    p = np.argwhere(abs(IR) == abs(IR).max()).squeeze()\n",
    "    Ipeak = IR[p]\n",
    "    while abs(Ipeak) > tol and i <= maxiter:\n",
    "        # Update the residual\n",
    "        IR -= gamma*Ipeak*Ipsf[int(Npsfmax - p):int(Npsfmax + Npix - p)]\n",
    "        # Update the sky model\n",
    "        IM[p] += gamma*Ipeak\n",
    "        # Find next peak\n",
    "        p = np.argwhere(abs(IR) == abs(IR).max()).squeeze()\n",
    "        Ipeak = IR[p]\n",
    "        i += 1\n",
    "        #print i, Ipeak, p\n",
    "    if i >= maxiter:\n",
    "        print(\"Warning maximum number of iterations exceeded\")\n",
    "    return IM, IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so lets try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ipsf = Ipsf2/maxPSF\n",
    "ID = ID2/maxPSF\n",
    "IM, IR = Hogbom_CLEAN(ID.copy(),Ipsf.copy(),tol=0.025)\n",
    "\n",
    "# Plot the results\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].stem(l,IM)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$I^{sky}$',fontsize=18)\n",
    "ax[0].set_xlim(lmin,lmax)\n",
    "\n",
    "\n",
    "ax[1].plot(l,IR)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^R$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is typical of Hogbom CLEAN. ALthough it has found the peaks in approximately the right locations, sometimes the flux gets spread out over neighbouring pixels. If we were to sum the fluxes from these pixels they would add up very nearly to the input flux. This is a limitation of working exclusively in image space where discretisation errors are inevitable. The Cotton-Schwab CLEAN algorithm overcomes this by performing the subtraction in visbility space. Unfortunately switching between image and visibility space is usually the most expensive part of the algorithm. It is therefore impractical to switch between image and visibility space each time the model gets updated. Sufficient accuracy can usually be obtained by only cleaning down to 0.1 or 0.2 of the peak in the residual image during each major cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CS_CLEAN(Vobs, F, Ipsf, peak_Factor=0.2, maxmajoriter=10,majortol = 0.01):\n",
    "    # Get max of the PSF and normalise\n",
    "    maxPSF = Ipsf.max()\n",
    "    Ipsf /= maxPSF \n",
    "    \n",
    "    # get the dirty image and normalise\n",
    "    ID = np.real(np.dot(F.conj().T,Vobs))/maxPSF\n",
    "    \n",
    "    # Initialise the model\n",
    "    IMmajor = np.zeros_like(ID)\n",
    "    \n",
    "    i = 0\n",
    "    IR = ID.copy()\n",
    "    \n",
    "    # Find the first peak\n",
    "    p = np.argwhere(abs(IR) == abs(IR).max()).squeeze()\n",
    "    Ipeak = IR[p]\n",
    "    while abs(Ipeak) > majortol and i <= maxmajoriter:        \n",
    "        # Do Hogbom step\n",
    "        IM, IR = Hogbom_CLEAN(IR,Ipsf,tol=peak_Factor*Ipeak)\n",
    "        \n",
    "        IMmajor += IM\n",
    "        \n",
    "        # Evaluate the Jacobian\n",
    "        Vpred = np.dot(F,IMmajor)\n",
    "        Vres = Vobs - Vpred\n",
    "        \n",
    "        # Get new residuals and normalise by max of PSF\n",
    "        IR = np.real(np.dot(F.conj().T,Vres))/maxPSF\n",
    "        \n",
    "        # Find the peak\n",
    "        p = np.argwhere(abs(IR) == abs(IR).max()).squeeze()\n",
    "        Ipeak = IR[p]  \n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    return IMmajor, IR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM, IR = CS_CLEAN(Vobs2, F2, Ipsf2.copy())\n",
    "\n",
    "# Plot the results\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].stem(l,IM)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$I^{sly}$',fontsize=18)\n",
    "ax[0].set_xlim(lmin,lmax)\n",
    "\n",
    "ax[1].plot(l,IR)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^R$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the CS CLEAN does a lot better at recovering the true fluxes of the sources. We have only one job left to do and that is to create the restored image. The restored image is the model image convolved with an ideal beam (i.e. one with no sidelobes). The ideal beam can be constructed by fitting a Gaussian to the primary lobe of the PSF.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_1D(x, sigma, l):\n",
    "    return sigma**2*np.exp(-x**2/(2*l**2))\n",
    "\n",
    "def fit_Gaussian_1D(Ipsf,lpsf):\n",
    "    I = np.argwhere(Ipsf > 0.5*Ipsf).squeeze()\n",
    "    data = Ipsf[I]\n",
    "    x = (lpsf[I])\n",
    "    \n",
    "    initial_guess = (1.0,0.5)\n",
    "    \n",
    "    popt, pcov = opt.curve_fit(Gaussian_1D, x, data, p0=initial_guess)\n",
    "    Ipsf_ideal = Gaussian_1D((lpsf), *popt)\n",
    "    Ipsf_ideal/=Ipsf_ideal.max()\n",
    "    \n",
    "    return Ipsf_ideal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ipsf_ideal = fit_Gaussian_1D(Ipsf,lpsf)\n",
    "\n",
    "# Convolve with sky model\n",
    "Irestored = scg.convolve(IM,Ipsf_ideal, mode='same')\n",
    "\n",
    "# Add back the residuals\n",
    "Irestored += IR\n",
    "\n",
    "# Plot the results\n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "\n",
    "ax[0].plot(lpsf,Ipsf_ideal)\n",
    "ax[0].set_xlabel('$l$')\n",
    "ax[0].set_ylabel('$\\hat{I}^{PSF}$',fontsize=18)\n",
    "ax[0].set_xlim(-1,1)\n",
    "\n",
    "ax[1].plot(l,Irestored)\n",
    "ax[1].set_xlabel('$l$')\n",
    "ax[1].set_ylabel('$I^r$',fontsize=18)\n",
    "ax[1].set_xlim(lmin,lmax)\n",
    "ax[1].set_ylim(0,3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14.3 Weighting and the PSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is incomplete. I thought it might be cool to demonstrate the effect of weighting for the 1D problem. Slider controls the degree of weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_PSF(S,F,u,R,l):\n",
    "    Ws = np.zeros(u.size)\n",
    "    I = np.argwhere(u<0)\n",
    "    Ws[I] = np.abs((-u[I])**R)\n",
    "    I = np.argwhere(u>=0)\n",
    "    Ws[I] = np.abs(u[I]**R)\n",
    "    Ipsf = np.real(np.dot(F.conj().T,S*Ws))\n",
    "    Ipsf /= Ipsf.max()\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(15,8))\n",
    "    ax[0].plot(l,Ipsf)\n",
    "    ax[0].set_xlim(-1,1)\n",
    "    ax[0].set_ylim(-0.1,1.1)\n",
    "    ax[0].set_title(r'$PSF$')\n",
    "    ax[1].plot(u,Ws)\n",
    "    ax[1].set_xlim(-20,20)\n",
    "    ax[1].set_ylim(0,80)\n",
    "    ax[0].set_title(r'$Visibility Weights$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(lambda R:inter_PSF(S=S2,F=F2psf,u=u[Sindices],R=R,l=lpsf),\n",
    "                R=(-1.5,1.5,0.25)) and None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next: [2.x Further Reading and References](2_x_further_reading_and_references.ipynb)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
